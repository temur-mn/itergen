{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SynthEval check: real vs synthetic\n",
        "\n",
        "This notebook evaluates a generated dataset against the given real Iris dataset using `SynthEval`.\n",
        "\n",
        "Focused metrics:\n",
        "- PCA eigenvalue difference (`pca_eigval_diff`, equivalent idea to `exp_var_diff`)\n",
        "- PCA first-component angle (`pca_eigvec_ang`, equivalent idea to `comp_angle_diff`)\n",
        "- Quantile MSE (`avg_qMSE`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from syntheval import SynthEval\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / 'demo').exists() and (ROOT.parent / 'demo').exists():\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "ROOT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real/given dataset (Iris)\n",
        "iris = load_iris(as_frame=True)\n",
        "real_df = iris.frame.rename(columns={\n",
        "    'sepal length (cm)': 'sepal_length',\n",
        "    'sepal width (cm)': 'sepal_width',\n",
        "    'petal length (cm)': 'petal_length',\n",
        "    'petal width (cm)': 'petal_width',\n",
        "}).copy()\n",
        "real_df['species'] = real_df['target'].map(dict(enumerate(iris.target_names)))\n",
        "real_df = real_df.drop(columns=['target'])\n",
        "\n",
        "real_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic datasets to test.\n",
        "# Keep at least one generated file. Add more if you have additional given synthetic files.\n",
        "SYNTHETIC_DATASETS = {\n",
        "    'itergen_generated': ROOT / 'demo' / 'output' / 'iris_synthetic.csv',\n",
        "    # 'given_dataset': ROOT / 'path' / 'to' / 'given_synthetic.csv',\n",
        "}\n",
        "\n",
        "loaded = {}\n",
        "for name, path in SYNTHETIC_DATASETS.items():\n",
        "    if Path(path).exists():\n",
        "        loaded[name] = pd.read_csv(path)\n",
        "    else:\n",
        "        print(f'Skipping {name}: file not found -> {path}')\n",
        "\n",
        "if not loaded:\n",
        "    raise FileNotFoundError('No synthetic datasets were found. Generate one first.')\n",
        "\n",
        "{k: v.shape for k, v in loaded.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_eval(real_frame: pd.DataFrame, synthetic_frame: pd.DataFrame) -> pd.DataFrame:\n",
        "    evaluator = SynthEval(real_frame, cat_cols=['species'], verbose=False)\n",
        "    return evaluator.evaluate(\n",
        "        synthetic_frame,\n",
        "        analysis_target_var='species',\n",
        "        pca={'preprocess': 'mean'},\n",
        "        q_mse={'num_quants': 10, 'cat_mse': False},\n",
        "    )\n",
        "\n",
        "reports = []\n",
        "for name, synthetic_df in loaded.items():\n",
        "    result = run_eval(real_df, synthetic_df)\n",
        "    result['dataset'] = name\n",
        "    reports.append(result)\n",
        "\n",
        "all_results = pd.concat(reports, ignore_index=True)\n",
        "all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric_map = {\n",
        "    'pca_eigval_diff': 'exp_var_diff_like',\n",
        "    'pca_eigvec_ang': 'comp_angle_diff_like',\n",
        "    'avg_qMSE': 'qMSE',\n",
        "}\n",
        "\n",
        "focus = all_results[all_results['metric'].isin(metric_map)].copy()\n",
        "focus['metric_alias'] = focus['metric'].map(metric_map)\n",
        "focus = focus[['dataset', 'metric', 'metric_alias', 'val', 'n_val', 'err', 'n_err']]\n",
        "focus.sort_values(['dataset', 'metric_alias'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: quick ranking (higher normalized value is better in SynthEval output).\n",
        "ranking = (\n",
        "    focus.groupby('dataset', as_index=False)['n_val']\n",
        "    .mean()\n",
        "    .rename(columns={'n_val': 'mean_normalized_score'})\n",
        "    .sort_values('mean_normalized_score', ascending=False)\n",
        ")\n",
        "ranking\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "itergen (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
