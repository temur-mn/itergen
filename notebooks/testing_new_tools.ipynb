{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5659fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CONFIG LOADED]\n",
      "  Name: sector_retail_shop_binary\n",
      "  Version: 1.0.0\n",
      "  Columns: 4\n",
      "\n",
      "[GENERATING 1000 ROWS]\n",
      "[INIT] has_loyalty_card: target_prob=0.350, n_true=350\n",
      "[INIT] discount_applied: target_prob=0.200, n_true=200\n",
      "[INIT] is_online_order: target_prob=0.420, n_true=420\n",
      "[INIT] is_return: target_prob=0.080, n_true=80\n",
      "\n",
      "[OBJECTIVE] OptimizationObjective(kl=0.4, tv=0.4, max_dev=0.2, cond_tol=0.05)\n",
      "\n",
      "[START] KL=0.010655, TV=0.036500, Score=0.044062\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.018759, KL=0.011089, TV=0.041500\n",
      "\n",
      "--- ITERATION 1 ---\n",
      "  KL=0.186301, TV=0.254500, Score=0.247520\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.011107, KL=0.013743, TV=0.055250\n",
      "\n",
      "--- ITERATION 2 ---\n",
      "  KL=0.014969, TV=0.058250, Score=0.058688\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 3 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 4 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 5 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 6 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 7 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 8 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 9 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 10 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 11 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 12 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 13 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 14 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 15 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 16 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 17 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 18 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 19 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 20 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 21 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 22 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 23 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 24 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 25 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 26 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 27 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 28 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 29 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 30 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 31 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 32 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 33 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 34 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 35 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 36 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 37 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 38 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 39 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 40 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 41 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 42 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 43 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 44 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 45 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 46 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 47 ---\n",
      "  KL=0.011493, TV=0.046500, Score=0.048397\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 48 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 49 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[ATTENTION PROPOSAL OPTIMIZATION]\n",
      "  Epoch 0: loss=0.002741, KL=0.010657, TV=0.037000\n",
      "\n",
      "--- ITERATION 50 ---\n",
      "  KL=0.011585, TV=0.046750, Score=0.048534\n",
      "  [REJECTED]\n",
      "\n",
      "[FINAL] KL=0.010655, TV=0.036500, Score=0.044062\n",
      "\n",
      "[FINAL DISTRIBUTION SUMMARY]\n",
      "  has_loyalty_card: target=0.350, observed=0.350, error=0.0000\n",
      "  discount_applied: target=0.200, observed=0.326, error=0.1260\n",
      "  is_online_order: target=0.420, observed=0.420, error=0.0000\n",
      "  is_return: target=0.080, observed=0.060, error=0.0200\n",
      "\n",
      "[FINAL DATASET SAMPLE]\n",
      "   has_loyalty_card  discount_applied  is_online_order  is_return\n",
      "0                 0                 0                0          0\n",
      "1                 0                 0                0          0\n",
      "2                 0                 0                1          0\n",
      "3                 0                 0                1          0\n",
      "4                 1                 1                0          0\n",
      "5                 0                 0                0          0\n",
      "6                 0                 0                0          0\n",
      "7                 1                 0                1          0\n",
      "8                 1                 0                0          0\n",
      "9                 0                 0                1          0\n",
      "\n",
      "[VALUE COUNTS PER COLUMN]\n",
      "\n",
      "has_loyalty_card:\n",
      "has_loyalty_card\n",
      "0    650\n",
      "1    350\n",
      "Name: count, dtype: int64\n",
      "\n",
      "discount_applied:\n",
      "discount_applied\n",
      "0    674\n",
      "1    326\n",
      "Name: count, dtype: int64\n",
      "\n",
      "is_online_order:\n",
      "is_online_order\n",
      "0    580\n",
      "1    420\n",
      "Name: count, dtype: int64\n",
      "\n",
      "is_return:\n",
      "is_return\n",
      "0    940\n",
      "1     60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[VALIDATION RESULTS]\n",
      "  has_loyalty_card: target=0.350, observed=0.350, [PASS]\n",
      "  discount_applied: target=0.200, observed=0.326, [FAIL]\n",
      "  is_online_order: target=0.420, observed=0.420, [PASS]\n",
      "  is_return: target=0.080, observed=0.060, [PASS]\n",
      "\n",
      "[METRICS COMPARISON]\n",
      "  MSE: 0.004069\n",
      "  KL Divergence: 0.010655\n",
      "  Total Variation: 0.036500\n",
      "  Max Deviation: 0.126000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary Synthetic Data Generator with Hybrid Rule + ML Architecture.\n",
    "\n",
    "This module generates and optimizes synthetic binary datasets to match target\n",
    "probability distributions defined in a YAML configuration file.\n",
    "\n",
    "Architecture Overview (Rec #1 - Hybrid Design):\n",
    "================================================\n",
    "The system uses a two-phase hybrid approach:\n",
    "\n",
    "1. StructuredAdjuster (Rules)\n",
    "   - Handles hard constraints\n",
    "   - Rules encode truth\n",
    "   - Gradient-based probability correction\n",
    "   - Row-level flipping preserves correlations\n",
    "\n",
    "2. AttentionProposalModel (ML)\n",
    "   - Handles soft, learned corrections\n",
    "   - ML encodes interaction uncertainty\n",
    "   - NOT a predictor - a learned proposal generator for optimization\n",
    "   - Used within hill-climbing optimization loop\n",
    "\n",
    "This separation prevents the model from \"learning the wrong thing\".\n",
    "\n",
    "Key Features:\n",
    "=============\n",
    "- Structured Adjustment: Gradient-based probability correction with\n",
    "  per-column learning rates (Rec #9) for rare variable stability\n",
    "\n",
    "- Attention-Based Proposals: Self-attention learns column interactions\n",
    "  and proposes probability corrections. Limited capacity (Rec #6) and\n",
    "  automatic freezing (Rec #7) prevent overfitting.\n",
    "\n",
    "- Better Metrics: KL divergence and total variation distance, plus\n",
    "  conditional metrics per dependency group (Rec #4)\n",
    "\n",
    "- Named Optimization Objective: Explicit weighted composite score (Rec #5)\n",
    "  prevents silent metric drift\n",
    "\n",
    "- Centralized RNG: Single RNGManager (Rec #12) for reproducibility\n",
    "\n",
    "- Invariant Checks: Fail-fast validation after each iteration (Rec #13)\n",
    "\n",
    "Main Entry Points:\n",
    "==================\n",
    "- ImprovedBinaryGenerator.generate(n_rows) - Full generation pipeline\n",
    "- optimize_dataset(df, config) - Repair/optimize existing datasets (Rec #15)\n",
    "\n",
    "Example Usage:\n",
    "==============\n",
    "    >>> config = load_config(CONFIG_YAML)\n",
    "    >>> generator = ImprovedBinaryGenerator(config, seed=42)\n",
    "    >>> df = generator.generate(n_rows=1000)\n",
    "\n",
    "    # Or optimize an existing dataset:\n",
    "    >>> df_fixed, metrics = optimize_dataset(existing_df, config)\n",
    "\"\"\"\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Random Number Generator Manager (Rec #12)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class RNGManager: # Clear\n",
    "    \"\"\"\n",
    "    Centralized random number generator manager (Rec #12).\n",
    "\n",
    "    Provides a single source of randomness for reproducibility.\n",
    "    Pass this to all components that need random number generation.\n",
    "\n",
    "    Benefits:\n",
    "    - Single seed controls all randomness\n",
    "    - No silent reproducibility bugs from multiple RNGs\n",
    "    - Easy to reset for testing\n",
    "    - Thread-safe within a single manager instance\n",
    "\n",
    "    Usage:\n",
    "        rng_manager = RNGManager(seed=42)\n",
    "        adjuster = StructuredAdjuster(config, rng_manager=rng_manager)\n",
    "        generator = ImprovedBinaryGenerator(config, rng_manager=rng_manager)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed: int = 42):\n",
    "        \"\"\"Initialize with a single seed.\"\"\"\n",
    "        self.seed = seed\n",
    "        self._numpy_rng = np.random.default_rng(seed)\n",
    "        self._torch_seed = seed\n",
    "        self._reset_torch()\n",
    "\n",
    "    def _reset_torch(self) -> None:\n",
    "        \"\"\"Reset PyTorch random state.\"\"\"\n",
    "        torch.manual_seed(self._torch_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self._torch_seed)\n",
    "\n",
    "    @property\n",
    "    def numpy(self) -> np.random.Generator:\n",
    "        \"\"\"Get the numpy random generator.\"\"\"\n",
    "        return self._numpy_rng\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None) -> None:\n",
    "        \"\"\"Reset all RNGs to initial state or new seed.\"\"\"\n",
    "        if seed is not None:\n",
    "            self.seed = seed\n",
    "            self._torch_seed = seed\n",
    "        self._numpy_rng = np.random.default_rng(self.seed)\n",
    "        self._reset_torch()\n",
    "\n",
    "    def choice(\n",
    "        self,\n",
    "        a: np.ndarray,\n",
    "        size: Optional[int] = None,\n",
    "        replace: bool = True,\n",
    "        p: Optional[np.ndarray] = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Wrapper for numpy random choice.\"\"\"\n",
    "        return self._numpy_rng.choice(a, size=size, replace=replace, p=p)\n",
    "\n",
    "    def shuffle(self, x: list) -> None:\n",
    "        \"\"\"In-place shuffle using numpy RNG.\"\"\"\n",
    "        self._numpy_rng.shuffle(x)\n",
    "\n",
    "    def random(self, size: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"Generate random floats in [0, 1).\"\"\"\n",
    "        return self._numpy_rng.random(size)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"RNGManager(seed={self.seed})\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration and Data Classes\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnConfig: # Clear\n",
    "    \"\"\"Stores parsed column configuration.\"\"\"\n",
    "\n",
    "    column_id: str\n",
    "    true_value: int\n",
    "    false_value: int\n",
    "    target_prob: float\n",
    "    distribution_type: str\n",
    "    dependencies: Optional[Dict] = None\n",
    "    conditional_probs: Optional[Dict] = None\n",
    "    # Rec #9: Per-column learning rate\n",
    "    learning_rate: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass # Clear\n",
    "class GeneratorMetrics:\n",
    "    \"\"\"\n",
    "    Stores evaluation metrics for the generator.\n",
    "\n",
    "    Includes both marginal metrics (overall distribution match) and\n",
    "    conditional metrics (per-dependency-group distribution match).\n",
    "    \"\"\"\n",
    "\n",
    "    mse: float\n",
    "    kl_divergence: float\n",
    "    total_variation: float\n",
    "    max_deviation: float\n",
    "    per_column_errors: Dict[str, float]\n",
    "    # Rec #4: Conditional metrics per dependency group\n",
    "    conditional_kl: Optional[Dict[str, float]] = None\n",
    "    conditional_tv: Optional[Dict[str, float]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizationObjective:\n",
    "    \"\"\"\n",
    "    Named composite objective for optimization (Rec #5).\n",
    "\n",
    "    Makes acceptance criteria explicit to avoid silent metric drift\n",
    "    when others modify the system. The objective is computed as:\n",
    "\n",
    "        objective = (weight_kl * kl) + (weight_tv * tv) + (weight_max_dev * max_dev)\n",
    "                    + conditional_penalty (if conditionals exceed tolerance)\n",
    "\n",
    "    Attributes:\n",
    "        weight_kl: Weight for KL divergence component\n",
    "        weight_tv: Weight for total variation component\n",
    "        weight_max_dev: Weight for maximum deviation penalty\n",
    "        conditional_tolerance: Max allowed conditional drift before penalty\n",
    "        conditional_penalty_weight: Penalty multiplier for conditional violations\n",
    "    \"\"\"\n",
    "\n",
    "    weight_kl: float = 0.4\n",
    "    weight_tv: float = 0.4\n",
    "    weight_max_dev: float = 0.2\n",
    "    conditional_tolerance: float = 0.05\n",
    "    conditional_penalty_weight: float = 0.5\n",
    "\n",
    "    def compute(self, metrics: GeneratorMetrics) -> float:\n",
    "        \"\"\"\n",
    "        Compute the weighted composite objective score.\n",
    "\n",
    "        Lower is better. Includes penalty for conditional distribution drift.\n",
    "        \"\"\"\n",
    "        base_score = (\n",
    "            self.weight_kl * metrics.kl_divergence\n",
    "            + self.weight_tv * metrics.total_variation\n",
    "            + self.weight_max_dev * metrics.max_deviation\n",
    "        )\n",
    "\n",
    "        # Add penalty for conditional distribution violations\n",
    "        conditional_penalty = 0.0\n",
    "        if metrics.conditional_tv:\n",
    "            violations = [\n",
    "                tv\n",
    "                for tv in metrics.conditional_tv.values()\n",
    "                if tv > self.conditional_tolerance\n",
    "            ]\n",
    "            if violations:\n",
    "                conditional_penalty = (\n",
    "                    self.conditional_penalty_weight * sum(violations) / len(violations)\n",
    "                )\n",
    "\n",
    "        return base_score + conditional_penalty\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return (\n",
    "            f\"OptimizationObjective(kl={self.weight_kl}, tv={self.weight_tv}, \"\n",
    "            f\"max_dev={self.weight_max_dev}, cond_tol={self.conditional_tolerance})\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Metrics Module - Better alternatives to MSE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DistributionMetrics:\n",
    "    \"\"\"\n",
    "    Provides multiple metrics for measuring how well generated distributions\n",
    "    match target probabilities. Offers alternatives to simple MSE.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(observed: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Mean Squared Error - original metric.\"\"\"\n",
    "        return float(np.mean((observed - target) ** 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_divergence(\n",
    "        observed: np.ndarray, target: np.ndarray, epsilon: float = 1e-10\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Kullback-Leibler Divergence - measures information loss when using\n",
    "        observed distribution to approximate target distribution.\n",
    "\n",
    "        Better than MSE because:\n",
    "        - Asymmetric: penalizes underrepresentation more heavily\n",
    "        - Information-theoretic interpretation\n",
    "        - More sensitive to rare events (important for binary vars with low prob)\n",
    "        \"\"\"\n",
    "        # Clip to avoid log(0)\n",
    "        observed = np.clip(observed, epsilon, 1 - epsilon)\n",
    "        target = np.clip(target, epsilon, 1 - epsilon)\n",
    "\n",
    "        # KL for binary: D(p||q) = p*log(p/q) + (1-p)*log((1-p)/(1-q))\n",
    "        kl = target * np.log(target / observed) + (1 - target) * np.log(\n",
    "            (1 - target) / (1 - observed)\n",
    "        )\n",
    "        return float(np.mean(kl))\n",
    "\n",
    "    @staticmethod\n",
    "    def total_variation(observed: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Total Variation Distance - maximum difference between distributions.\n",
    "\n",
    "        Better than MSE because:\n",
    "        - Bounded between 0 and 1\n",
    "        - Directly interpretable as \"how different are the distributions\"\n",
    "        - For binary: TV = |p_obs - p_target|\n",
    "        \"\"\"\n",
    "        return float(np.mean(np.abs(observed - target)))\n",
    "\n",
    "    @staticmethod\n",
    "    def max_deviation(observed: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Maximum absolute deviation across all columns.\"\"\"\n",
    "        return float(np.max(np.abs(observed - target)))\n",
    "\n",
    "    @staticmethod\n",
    "    def chi_squared(\n",
    "        observed_counts: np.ndarray, expected_counts: np.ndarray, epsilon: float = 1e-10\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Chi-squared statistic for goodness of fit.\n",
    "\n",
    "        Better for categorical data as it accounts for sample size.\n",
    "        \"\"\"\n",
    "        expected_counts = np.clip(expected_counts, epsilon, None)\n",
    "        chi2 = np.sum((observed_counts - expected_counts) ** 2 / expected_counts)\n",
    "        return float(chi2)\n",
    "\n",
    "    @staticmethod\n",
    "    def composite_score(\n",
    "        observed: np.ndarray,\n",
    "        target: np.ndarray,\n",
    "        weights: Optional[Dict[str, float]] = None,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Weighted combination of metrics for comprehensive evaluation.\n",
    "\n",
    "        Default weights emphasize KL divergence and total variation.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {\"kl\": 0.4, \"tv\": 0.4, \"max_dev\": 0.2}\n",
    "\n",
    "        metrics = DistributionMetrics\n",
    "        score = (\n",
    "            weights.get(\"kl\", 0) * metrics.kl_divergence(observed, target)\n",
    "            + weights.get(\"tv\", 0) * metrics.total_variation(observed, target)\n",
    "            + weights.get(\"max_dev\", 0) * metrics.max_deviation(observed, target)\n",
    "        )\n",
    "        return score\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Self-Attention Module with Backpropagation\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class BinaryAttentionAdjuster(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention mechanism for learning inter-column relationships\n",
    "    and adjusting probabilities through backpropagation.\n",
    "\n",
    "    The attention mechanism learns:\n",
    "    1. Which columns influence each other\n",
    "    2. How to adjust probabilities to minimize distribution error\n",
    "    3. Optimal correction factors through gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_columns: int, hidden_dim: int = 32, n_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.n_columns = n_columns\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Learnable probability adjustment parameters\n",
    "        self.prob_adjustments = nn.Parameter(torch.zeros(n_columns))\n",
    "\n",
    "        # Self-attention layers\n",
    "        self.query = nn.Linear(1, hidden_dim)\n",
    "        self.key = nn.Linear(1, hidden_dim)\n",
    "        self.value = nn.Linear(1, hidden_dim)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, num_heads=n_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Output projection to probability adjustment\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Tanh(),  # Bound adjustments to [-1, 1]\n",
    "        )\n",
    "\n",
    "        # Scale factor for adjustments (learnable)\n",
    "        self.adjustment_scale = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(\n",
    "        self, current_probs: torch.Tensor, target_probs: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute probability adjustments using self-attention.\n",
    "\n",
    "        Args:\n",
    "            current_probs: Current observed probabilities [n_columns]\n",
    "            target_probs: Target probabilities [n_columns]\n",
    "\n",
    "        Returns:\n",
    "            Adjusted probabilities [n_columns]\n",
    "        \"\"\"\n",
    "        # Compute error signal\n",
    "        error = target_probs - current_probs  # [n_columns]\n",
    "\n",
    "        # Reshape for attention: [1, n_columns, 1]\n",
    "        x = error.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        # Project to query, key, value\n",
    "        q = self.query(x)  # [1, n_columns, hidden_dim]\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Self-attention to learn column interactions\n",
    "        attended, attention_weights = self.attention(q, k, v)\n",
    "\n",
    "        # Project to adjustment values\n",
    "        adjustments = self.output_proj(attended).squeeze(-1).squeeze(0)  # [n_columns]\n",
    "\n",
    "        # Scale adjustments\n",
    "        adjustments = adjustments * self.adjustment_scale\n",
    "\n",
    "        # Add learnable base adjustments\n",
    "        adjustments = adjustments + self.prob_adjustments\n",
    "\n",
    "        # Apply adjustments to current probabilities\n",
    "        adjusted_probs = current_probs + adjustments\n",
    "\n",
    "        # Clamp to valid probability range\n",
    "        adjusted_probs = torch.clamp(adjusted_probs, 0.0, 1.0)\n",
    "\n",
    "        return adjusted_probs, attention_weights\n",
    "\n",
    "\n",
    "class AttentionProposalModel:\n",
    "    \"\"\"\n",
    "    Learned proposal generator for probability corrections in optimization.\n",
    "\n",
    "    IMPORTANT: This model is NOT used for inference or prediction.\n",
    "    It is used exclusively for optimization guidance - learning how to propose\n",
    "    probability corrections within a hill-climbing optimization loop.\n",
    "\n",
    "    The model learns:\n",
    "    - How to propose probability adjustments that minimize distribution error\n",
    "    - Optimal correction magnitudes through gradient descent\n",
    "    - Column interaction patterns that inform adjustment proposals\n",
    "\n",
    "    This is part of a hybrid rule + ML architecture where:\n",
    "    - StructuredAdjuster handles hard constraints (rules encode truth)\n",
    "    - AttentionProposalModel handles soft, learned corrections (ML encodes uncertainty)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_columns: int,\n",
    "        learning_rate: float = 0.01,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        n_heads: Optional[int] = None,\n",
    "    ):\n",
    "        # Rec #6: Limit attention capacity based on column count\n",
    "        # Rule of thumb: hidden_dim <= 4 * n_columns, n_heads <= 2 for n_columns < 10\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = min(32, 4 * n_columns)\n",
    "        if n_heads is None:\n",
    "            n_heads = 2 if n_columns < 10 else 4\n",
    "\n",
    "        self.model = BinaryAttentionAdjuster(\n",
    "            n_columns=n_columns, hidden_dim=hidden_dim, n_heads=n_heads\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_history = []\n",
    "\n",
    "        # Rec #7: Freezing state tracking\n",
    "        self.frozen = False\n",
    "        self.early_stop_patience = 10\n",
    "        self.early_stop_min_delta = 1e-6\n",
    "        self._no_improvement_count = 0\n",
    "        self._best_loss = float(\"inf\")\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Freeze the model after convergence (Rec #7).\n",
    "\n",
    "        Once frozen, the model will only apply learned adjustments\n",
    "        without further training. This prevents:\n",
    "        - Chasing noise during continued optimization\n",
    "        - Destabilizing already-converged solutions\n",
    "        \"\"\"\n",
    "        self.frozen = True\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def is_frozen(self) -> bool:\n",
    "        \"\"\"Check if model is frozen.\"\"\"\n",
    "        return self.frozen\n",
    "\n",
    "    def _check_early_stopping(self, loss: float) -> bool:\n",
    "        \"\"\"\n",
    "        Check if training should stop due to convergence (Rec #7).\n",
    "\n",
    "        Returns True if model should be frozen.\n",
    "        \"\"\"\n",
    "        if loss < self._best_loss - self.early_stop_min_delta:\n",
    "            self._best_loss = loss\n",
    "            self._no_improvement_count = 0\n",
    "        else:\n",
    "            self._no_improvement_count += 1\n",
    "\n",
    "        return self._no_improvement_count >= self.early_stop_patience\n",
    "\n",
    "    def compute_loss(\n",
    "        self, adjusted_probs: torch.Tensor, target_probs: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute differentiable loss combining multiple metrics.\n",
    "\n",
    "        Uses a combination of:\n",
    "        - MSE loss (smooth gradients)\n",
    "        - Soft KL-like loss (penalizes rare event errors)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10\n",
    "\n",
    "        # MSE component\n",
    "        mse_loss = F.mse_loss(adjusted_probs, target_probs)\n",
    "\n",
    "        # KL-like component (symmetric version for stability)\n",
    "        p = torch.clamp(adjusted_probs, epsilon, 1 - epsilon)\n",
    "        q = torch.clamp(target_probs, epsilon, 1 - epsilon)\n",
    "\n",
    "        kl_forward = q * torch.log(q / p) + (1 - q) * torch.log((1 - q) / (1 - p))\n",
    "        kl_backward = p * torch.log(p / q) + (1 - p) * torch.log((1 - p) / (1 - q))\n",
    "        kl_symmetric = 0.5 * (kl_forward + kl_backward).mean()\n",
    "\n",
    "        # Total variation component\n",
    "        tv_loss = torch.abs(adjusted_probs - target_probs).mean()\n",
    "\n",
    "        # Combined loss with weights\n",
    "        total_loss = 0.3 * mse_loss + 0.4 * kl_symmetric + 0.3 * tv_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def train_step(\n",
    "        self, current_probs: np.ndarray, target_probs: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Perform one training step with backpropagation.\n",
    "\n",
    "        Args:\n",
    "            current_probs: Currently observed probabilities\n",
    "            target_probs: Target probabilities from config\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (adjusted_probs, loss_value)\n",
    "\n",
    "        Note: If the model is frozen (Rec #7), this will only apply\n",
    "        learned adjustments without training.\n",
    "        \"\"\"\n",
    "        # Convert to tensors\n",
    "        current_t = torch.tensor(current_probs, dtype=torch.float32)\n",
    "        target_t = torch.tensor(target_probs, dtype=torch.float32)\n",
    "\n",
    "        # Rec #7: If frozen, only apply adjustments without training\n",
    "        if self.frozen:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                adjusted_probs, _ = self.model(current_t, target_t)\n",
    "            return adjusted_probs.numpy(), self._best_loss\n",
    "\n",
    "        # Normal training path\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        adjusted_probs, attention_weights = self.model(current_t, target_t)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(adjusted_probs, target_t)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "\n",
    "        # Rec #7: Check for early stopping and freeze if converged\n",
    "        if self._check_early_stopping(loss_value):\n",
    "            self.freeze()\n",
    "\n",
    "        return adjusted_probs.detach().numpy(), loss_value\n",
    "\n",
    "    def get_adjustments(\n",
    "        self, current_probs: np.ndarray, target_probs: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Get probability adjustments without training.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_t = torch.tensor(current_probs, dtype=torch.float32)\n",
    "            target_t = torch.tensor(target_probs, dtype=torch.float32)\n",
    "            adjusted_probs, _ = self.model(current_t, target_t)\n",
    "        return adjusted_probs.numpy()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Structured Adjustment Mechanism\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class StructuredAdjuster:\n",
    "    \"\"\"\n",
    "    Implements structured probability adjustment for binary variables.\n",
    "\n",
    "    Instead of random jitter, uses:\n",
    "    1. Gradient-based correction: adjust proportionally to error\n",
    "    2. Constraint satisfaction: maintain conditional dependencies\n",
    "    3. Iterative refinement: progressively reduce error\n",
    "\n",
    "    Part of a hybrid rule + ML architecture (Rec #1):\n",
    "    - This class handles hard constraints (rules encode truth)\n",
    "    - AttentionProposalModel handles soft, learned corrections\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        rng_manager: Optional[RNGManager] = None,\n",
    "        seed: int = 42,\n",
    "        default_learning_rate: float = 0.5,\n",
    "        column_learning_rates: Optional[Dict[str, float]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the structured adjuster.\n",
    "\n",
    "        Args:\n",
    "            config: YAML configuration dictionary\n",
    "            rng_manager: Centralized RNG manager (Rec #12). If None, creates one.\n",
    "            seed: Random seed (used only if rng_manager is None)\n",
    "            default_learning_rate: Default learning rate for all columns\n",
    "            column_learning_rates: Optional per-column learning rates (Rec #9)\n",
    "                Rare variables (e.g., returns with ~8% probability) benefit from\n",
    "                smaller learning rates for stability.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        # Rec #12: Use centralized RNG manager\n",
    "        self._rng_manager = rng_manager or RNGManager(seed)\n",
    "        self.default_learning_rate = default_learning_rate\n",
    "        self.column_learning_rates = column_learning_rates or {}\n",
    "        self.column_configs = self._parse_columns()\n",
    "        self.adjustment_history = []\n",
    "\n",
    "    @property\n",
    "    def rng(self) -> np.random.Generator:\n",
    "        \"\"\"Get the numpy RNG from the manager.\"\"\"\n",
    "        return self._rng_manager.numpy\n",
    "\n",
    "    def _parse_columns(self) -> Dict[str, ColumnConfig]:\n",
    "        \"\"\"Parse column configurations from YAML config.\"\"\"\n",
    "        columns = {}\n",
    "        for col in self.config[\"columns\"]:\n",
    "            col_id = col[\"column_id\"]\n",
    "            dist = col[\"distribution\"]\n",
    "\n",
    "            deps = None\n",
    "            cond_probs = None\n",
    "            if dist[\"type\"] == \"conditional\":\n",
    "                deps = dist[\"dependencies\"][\"depend_on\"]\n",
    "                cond_probs = dist[\"dependencies\"][\"conditional_probs\"]\n",
    "\n",
    "            # Rec #9: Determine column-specific learning rate\n",
    "            # Rare variables (low target_prob) get smaller learning rates\n",
    "            target_prob = dist[\"probabilities\"][\"true_prob\"]\n",
    "            if col_id in self.column_learning_rates:\n",
    "                lr = self.column_learning_rates[col_id]\n",
    "            elif target_prob < 0.1:  # Rare variable\n",
    "                lr = self.default_learning_rate * 0.5\n",
    "            elif target_prob > 0.9:  # Very common variable\n",
    "                lr = self.default_learning_rate * 0.5\n",
    "            else:\n",
    "                lr = self.default_learning_rate\n",
    "\n",
    "            columns[col_id] = ColumnConfig(\n",
    "                column_id=col_id,\n",
    "                true_value=col[\"values\"][\"true_value\"],\n",
    "                false_value=col[\"values\"][\"false_value\"],\n",
    "                target_prob=target_prob,\n",
    "                distribution_type=dist[\"type\"],\n",
    "                dependencies=deps,\n",
    "                conditional_probs=cond_probs,\n",
    "                learning_rate=lr,\n",
    "            )\n",
    "        return columns\n",
    "\n",
    "    def compute_error(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Compute per-column probability errors.\"\"\"\n",
    "        errors = {}\n",
    "        for col_id, cfg in self.column_configs.items():\n",
    "            observed = (df[col_id] == cfg.true_value).mean()\n",
    "            errors[col_id] = cfg.target_prob - observed\n",
    "        return errors\n",
    "\n",
    "    def compute_gradient_adjustment(\n",
    "        self, df: pd.DataFrame, learning_rate: Optional[float] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute structured adjustment factors using gradient-like updates.\n",
    "\n",
    "        For binary variables:\n",
    "        - If observed_prob < target_prob: need to flip some 0s to 1s\n",
    "        - If observed_prob > target_prob: need to flip some 1s to 0s\n",
    "\n",
    "        The adjustment is proportional to the error magnitude.\n",
    "\n",
    "        Uses per-column learning rates (Rec #9) where rare variables\n",
    "        get smaller learning rates for stability.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to compute adjustments for\n",
    "            learning_rate: Override learning rate (uses per-column rates if None)\n",
    "        \"\"\"\n",
    "        errors = self.compute_error(df)\n",
    "        adjustments = {}\n",
    "\n",
    "        for col_id, error in errors.items():\n",
    "            cfg = self.column_configs[col_id]\n",
    "\n",
    "            # Rec #9: Use column-specific learning rate\n",
    "            if learning_rate is not None:\n",
    "                col_lr = learning_rate\n",
    "            elif cfg.learning_rate is not None:\n",
    "                col_lr = cfg.learning_rate\n",
    "            else:\n",
    "                col_lr = self.default_learning_rate\n",
    "\n",
    "            # Gradient-based adjustment factor\n",
    "            # Positive error = need more 1s, negative error = need more 0s\n",
    "            adjustment = col_lr * error\n",
    "\n",
    "            # Adaptive learning rate: larger adjustments for larger errors\n",
    "            if abs(error) > 0.1:\n",
    "                adjustment *= 1.5\n",
    "            elif abs(error) < 0.01:\n",
    "                adjustment *= 0.5\n",
    "\n",
    "            adjustments[col_id] = adjustment\n",
    "\n",
    "        self.adjustment_history.append(adjustments)\n",
    "        return adjustments\n",
    "\n",
    "    def apply_structured_adjustment(\n",
    "        self, df: pd.DataFrame, adjustments: Dict[str, float]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply structured adjustments by selectively flipping binary values.\n",
    "\n",
    "        Uses targeted flipping rather than random re-generation:\n",
    "        - Flip values that most reduce error\n",
    "        - Respect conditional dependencies\n",
    "        - Maintain overall distribution shape\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        for col_id, adjustment in adjustments.items():\n",
    "            if abs(adjustment) < 1e-6:\n",
    "                continue\n",
    "\n",
    "            cfg = self.column_configs[col_id]\n",
    "            n_rows = len(df)\n",
    "\n",
    "            # Calculate how many rows need to be flipped\n",
    "            n_to_flip = int(abs(adjustment) * n_rows)\n",
    "            if n_to_flip == 0:\n",
    "                continue\n",
    "\n",
    "            if adjustment > 0:\n",
    "                # Need to flip 0s to 1s\n",
    "                candidates = df.index[df[col_id] == cfg.false_value].tolist()\n",
    "            else:\n",
    "                # Need to flip 1s to 0s\n",
    "                candidates = df.index[df[col_id] == cfg.true_value].tolist()\n",
    "\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "\n",
    "            # Select candidates to flip (with some randomness for diversity)\n",
    "            n_to_flip = min(n_to_flip, len(candidates))\n",
    "            flip_indices = self.rng.choice(candidates, size=n_to_flip, replace=False)\n",
    "\n",
    "            # Perform the flip\n",
    "            current_val = df.loc[flip_indices[0], col_id]\n",
    "            new_val = (\n",
    "                cfg.true_value if current_val == cfg.false_value else cfg.false_value\n",
    "            )\n",
    "            df.loc[flip_indices, col_id] = new_val\n",
    "\n",
    "        return df\n",
    "\n",
    "    def adjust_with_constraints(\n",
    "        self, df: pd.DataFrame, max_iterations: int = 10\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply adjustments while respecting conditional dependencies.\n",
    "\n",
    "        Iteratively adjusts until constraints are satisfied or max_iterations reached.\n",
    "        \"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            adjustments = self.compute_gradient_adjustment(df)\n",
    "\n",
    "            # Check if converged\n",
    "            max_adjustment = max(abs(a) for a in adjustments.values())\n",
    "            if max_adjustment < 0.001:\n",
    "                break\n",
    "\n",
    "            df = self.apply_structured_adjustment(df, adjustments)\n",
    "\n",
    "            # Re-apply conditional dependencies\n",
    "            df = self._enforce_conditional_dependencies(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _enforce_conditional_dependencies(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enforce conditional probability constraints.\n",
    "\n",
    "        Refactored to minimize Python loops by:\n",
    "        - Pre-building group masks using vectorized operations\n",
    "        - Using numpy arrays for candidate selection\n",
    "        - Batch updating values where possible\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        for col_id, cfg in self.column_configs.items():\n",
    "            if cfg.distribution_type != \"conditional\":\n",
    "                continue\n",
    "\n",
    "            if cfg.conditional_probs is None:\n",
    "                continue\n",
    "\n",
    "            # Rec #11: Pre-compute column values as numpy arrays for speed\n",
    "            col_values = np.asarray(df[col_id].values)\n",
    "\n",
    "            for condition_key, probs in cfg.conditional_probs.items():\n",
    "                cond = self._parse_condition_key(condition_key)\n",
    "\n",
    "                mask = np.ones(len(df), dtype=bool)\n",
    "                for k, v in cond.items():\n",
    "                    mask &= np.asarray(df[k].values) == v\n",
    "\n",
    "                n = int(mask.sum())\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get indices where mask is True\n",
    "                idx = np.where(mask)[0]\n",
    "\n",
    "                # Compute target count for this group\n",
    "                target_prob = probs[\"true_prob\"]\n",
    "                target_count = int(round(n * target_prob))\n",
    "\n",
    "                # Current count using vectorized operation\n",
    "                group_values = col_values[mask]\n",
    "                current_count = int((group_values == cfg.true_value).sum())\n",
    "\n",
    "                # Adjust if needed\n",
    "                diff = target_count - current_count\n",
    "                if diff > 0:\n",
    "                    # Need more 1s - find candidates with false_value\n",
    "                    candidates_mask = np.asarray(group_values == cfg.false_value)\n",
    "                    candidates = idx[candidates_mask]\n",
    "                    n_flip = min(diff, len(candidates))\n",
    "                    if n_flip > 0:\n",
    "                        flip_idx = self.rng.choice(\n",
    "                            candidates, size=n_flip, replace=False\n",
    "                        )\n",
    "                        df.iloc[flip_idx, df.columns.get_loc(col_id)] = cfg.true_value\n",
    "                elif diff < 0:\n",
    "                    # Need fewer 1s - find candidates with true_value\n",
    "                    candidates_mask = np.asarray(group_values == cfg.true_value)\n",
    "                    candidates = idx[candidates_mask]\n",
    "                    n_flip = min(-diff, len(candidates))\n",
    "                    if n_flip > 0:\n",
    "                        flip_idx = self.rng.choice(\n",
    "                            candidates, size=n_flip, replace=False\n",
    "                        )\n",
    "                        df.iloc[flip_idx, df.columns.get_loc(col_id)] = cfg.false_value\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_condition_key(condition_key: str) -> Dict[str, int]:\n",
    "        \"\"\"Parse condition key like 'is_online_order=1, has_loyalty_card=0'.\"\"\"\n",
    "        result = {}\n",
    "        for part in condition_key.split(\",\"):\n",
    "            key, value = part.strip().split(\"=\")\n",
    "            result[key] = int(value)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Generator Class with All Improvements\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class ImprovedBinaryGenerator:\n",
    "    \"\"\"\n",
    "    Main generator class combining all improvements:\n",
    "    1. Structured adjustment mechanism (rules encode truth)\n",
    "    2. Self-attention with backpropagation (ML encodes uncertainty)\n",
    "    3. Better metrics (KL, TV, composite)\n",
    "\n",
    "    This is a hybrid rule + ML architecture (Rec #1) that:\n",
    "    - Uses StructuredAdjuster for hard constraints\n",
    "    - Uses AttentionProposalModel for soft, learned corrections\n",
    "\n",
    "    The system excels at repairing/optimizing distributions,\n",
    "    not just initial sampling (see optimize_dataset for direct API).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        seed: int = 42,\n",
    "        rng_manager: Optional[RNGManager] = None,\n",
    "        use_attention: bool = True,\n",
    "        learning_rate: float = 0.01,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the generator.\n",
    "\n",
    "        Args:\n",
    "            config: YAML configuration dictionary\n",
    "            seed: Random seed (used only if rng_manager is None)\n",
    "            rng_manager: Centralized RNG manager (Rec #12). If None, creates one.\n",
    "            use_attention: Whether to use attention-based proposal model\n",
    "            learning_rate: Learning rate for attention model\n",
    "            verbose: Whether to print progress messages\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.seed = seed\n",
    "        self.use_attention = use_attention\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Rec #12: Use centralized RNG manager\n",
    "        self._rng_manager = rng_manager or RNGManager(seed)\n",
    "\n",
    "        # Initialize structured adjuster with shared RNG\n",
    "        self.adjuster = StructuredAdjuster(config, rng_manager=self._rng_manager)\n",
    "\n",
    "        # Initialize attention proposal model if enabled\n",
    "        n_columns = len(config[\"columns\"])\n",
    "        self.attention_model = (\n",
    "            AttentionProposalModel(n_columns=n_columns, learning_rate=learning_rate)\n",
    "            if use_attention\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Metrics calculator\n",
    "        self.metrics = DistributionMetrics()\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\"mse\": [], \"kl\": [], \"tv\": [], \"composite\": []}\n",
    "\n",
    "    @property\n",
    "    def rng(self) -> np.random.Generator:\n",
    "        \"\"\"Get the numpy RNG from the manager (Rec #12).\"\"\"\n",
    "        return self._rng_manager.numpy\n",
    "\n",
    "    def log(self, message: str):\n",
    "        \"\"\"Print message if verbose mode is on.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(message)\n",
    "\n",
    "    def generate_initial_dataset(self, n_rows: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate initial dataset ignoring conditional logic.\"\"\"\n",
    "        data = {}\n",
    "\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            values = col_cfg[\"values\"]\n",
    "            probs = col_cfg[\"distribution\"][\"probabilities\"]\n",
    "\n",
    "            true_val = values[\"true_value\"]\n",
    "            false_val = values[\"false_value\"]\n",
    "            p = probs[\"true_prob\"]\n",
    "\n",
    "            # Generate exact counts for target probability\n",
    "            n_true = int(round(n_rows * p))\n",
    "            column = [true_val] * n_true + [false_val] * (n_rows - n_true)\n",
    "\n",
    "            self.rng.shuffle(column)\n",
    "            data[col_id] = column\n",
    "            self.log(f\"[INIT] {col_id}: target_prob={p:.3f}, n_true={n_true}\")\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def evaluate(self, df: pd.DataFrame) -> GeneratorMetrics:\n",
    "        \"\"\"\n",
    "        Evaluate dataset against target distributions using multiple metrics.\n",
    "\n",
    "        Includes both marginal metrics and conditional metrics per dependency group.\n",
    "        Conditional metrics track distribution match within each conditional slice,\n",
    "        e.g., P(is_return | online=1, loyalty=1).\n",
    "        \"\"\"\n",
    "        observed = []\n",
    "        target = []\n",
    "        per_column = {}\n",
    "\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "            p_target = col_cfg[\"distribution\"][\"probabilities\"][\"true_prob\"]\n",
    "\n",
    "            p_observed = (df[col_id] == true_val).mean()\n",
    "            observed.append(p_observed)\n",
    "            target.append(p_target)\n",
    "            per_column[col_id] = abs(p_observed - p_target)\n",
    "\n",
    "        observed = np.array(observed)\n",
    "        target = np.array(target)\n",
    "\n",
    "        # Rec #4: Compute conditional metrics per dependency group\n",
    "        conditional_kl, conditional_tv = self._compute_conditional_metrics(df)\n",
    "\n",
    "        return GeneratorMetrics(\n",
    "            mse=self.metrics.mse(observed, target),\n",
    "            kl_divergence=self.metrics.kl_divergence(observed, target),\n",
    "            total_variation=self.metrics.total_variation(observed, target),\n",
    "            max_deviation=self.metrics.max_deviation(observed, target),\n",
    "            per_column_errors=per_column,\n",
    "            conditional_kl=conditional_kl,\n",
    "            conditional_tv=conditional_tv,\n",
    "        )\n",
    "\n",
    "    def _compute_conditional_metrics(\n",
    "        self, df: pd.DataFrame\n",
    "    ) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute KL and TV metrics for each conditional slice.\n",
    "\n",
    "        Tracks metrics per conditional group, e.g.:\n",
    "        - \"is_return|is_online_order=1,has_loyalty_card=1\"\n",
    "\n",
    "        This catches distribution errors that hide inside conditionals.\n",
    "        \"\"\"\n",
    "        conditional_kl = {}\n",
    "        conditional_tv = {}\n",
    "        epsilon = 1e-10\n",
    "\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            dist = col_cfg[\"distribution\"]\n",
    "            if dist[\"type\"] != \"conditional\":\n",
    "                continue\n",
    "\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "            cond_probs = dist[\"dependencies\"].get(\"conditional_probs\", {})\n",
    "\n",
    "            for condition_key, probs in cond_probs.items():\n",
    "                # Parse condition\n",
    "                cond = StructuredAdjuster._parse_condition_key(condition_key)\n",
    "\n",
    "                # Build mask for this condition\n",
    "                mask = pd.Series(True, index=df.index)\n",
    "                for k, v in cond.items():\n",
    "                    mask &= df[k] == v\n",
    "\n",
    "                subset = df[mask]\n",
    "                n = len(subset)\n",
    "                if n == 0:\n",
    "                    continue\n",
    "\n",
    "                p_target = probs[\"true_prob\"]\n",
    "                p_observed = (subset[col_id] == true_val).mean()\n",
    "\n",
    "                # Clip for numerical stability\n",
    "                p_obs_clipped = np.clip(p_observed, epsilon, 1 - epsilon)\n",
    "                p_tgt_clipped = np.clip(p_target, epsilon, 1 - epsilon)\n",
    "\n",
    "                # KL divergence for binary\n",
    "                kl = p_tgt_clipped * np.log(p_tgt_clipped / p_obs_clipped) + (\n",
    "                    1 - p_tgt_clipped\n",
    "                ) * np.log((1 - p_tgt_clipped) / (1 - p_obs_clipped))\n",
    "                tv = abs(p_observed - p_target)\n",
    "\n",
    "                metric_key = f\"{col_id}|{condition_key}\"\n",
    "                conditional_kl[metric_key] = float(kl)\n",
    "                conditional_tv[metric_key] = float(tv)\n",
    "\n",
    "        return conditional_kl, conditional_tv\n",
    "\n",
    "    def get_current_probs(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Get current observed probabilities for all columns.\"\"\"\n",
    "        probs = []\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "            probs.append((df[col_id] == true_val).mean())\n",
    "        return np.array(probs)\n",
    "\n",
    "    def get_target_probs(self) -> np.ndarray:\n",
    "        \"\"\"Get target probabilities from config.\"\"\"\n",
    "        return np.array(\n",
    "            [\n",
    "                col[\"distribution\"][\"probabilities\"][\"true_prob\"]\n",
    "                for col in self.config[\"columns\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def optimize_with_attention(\n",
    "        self, df: pd.DataFrame, n_epochs: int = 100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Optimize dataset using attention-based proposal model with backpropagation.\n",
    "\n",
    "        Note: The attention model is a learned proposal generator, not a predictor.\n",
    "        It learns how to propose probability corrections within the optimization loop.\n",
    "        \"\"\"\n",
    "        if self.attention_model is None:\n",
    "            return df\n",
    "\n",
    "        df = df.copy()\n",
    "        target_probs = self.get_target_probs()\n",
    "\n",
    "        self.log(\"\\n[ATTENTION PROPOSAL OPTIMIZATION]\")\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            current_probs = self.get_current_probs(df)\n",
    "\n",
    "            # Train attention proposal model\n",
    "            adjusted_probs, loss = self.attention_model.train_step(\n",
    "                current_probs, target_probs\n",
    "            )\n",
    "\n",
    "            # Compute adjustment factors\n",
    "            adjustments = {}\n",
    "            col_ids = [col[\"column_id\"] for col in self.config[\"columns\"]]\n",
    "            for i, col_id in enumerate(col_ids):\n",
    "                adjustments[col_id] = adjusted_probs[i] - current_probs[i]\n",
    "\n",
    "            # Apply adjustments\n",
    "            df = self.adjuster.apply_structured_adjustment(df, adjustments)\n",
    "\n",
    "            # Enforce conditional dependencies\n",
    "            df = self.adjuster._enforce_conditional_dependencies(df)\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                metrics = self.evaluate(df)\n",
    "                self.log(\n",
    "                    f\"  Epoch {epoch}: loss={loss:.6f}, KL={metrics.kl_divergence:.6f}, TV={metrics.total_variation:.6f}\"\n",
    "                )\n",
    "\n",
    "            # Early stopping\n",
    "            if loss < 1e-6:\n",
    "                self.log(f\"  Converged at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        max_iters: int = 50,\n",
    "        tolerance: float = 1e-6,\n",
    "        objective: Optional[OptimizationObjective] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main optimization loop combining structured adjustment and attention.\n",
    "\n",
    "        Uses a named composite objective (Rec #5) for explicit acceptance criteria.\n",
    "        This prevents silent metric drift when the system is modified.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame to optimize\n",
    "            max_iters: Maximum optimization iterations\n",
    "            tolerance: Convergence tolerance for objective score\n",
    "            objective: Custom optimization objective (uses default if None)\n",
    "\n",
    "        Returns:\n",
    "            Optimized DataFrame with distributions matching targets\n",
    "        \"\"\"\n",
    "        # Rec #5: Use explicit named objective instead of implicit score = kl + tv\n",
    "        if objective is None:\n",
    "            objective = OptimizationObjective()\n",
    "\n",
    "        self.log(f\"\\n[OBJECTIVE] {objective}\")\n",
    "\n",
    "        best_df = df.copy()\n",
    "        best_metrics = self.evaluate(best_df)\n",
    "        best_score = objective.compute(best_metrics)\n",
    "\n",
    "        # Rec #10: Track for early exit on no-op\n",
    "        prev_score = best_score\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        self.log(\n",
    "            f\"\\n[START] KL={best_metrics.kl_divergence:.6f}, \"\n",
    "            f\"TV={best_metrics.total_variation:.6f}, Score={best_score:.6f}\"\n",
    "        )\n",
    "\n",
    "        for it in range(1, max_iters + 1):\n",
    "            # Phase 1: Structured adjustment\n",
    "            candidate = self.adjuster.adjust_with_constraints(best_df, max_iterations=5)\n",
    "\n",
    "            # Phase 2: Attention-based refinement (if enabled)\n",
    "            if self.use_attention:\n",
    "                candidate = self.optimize_with_attention(candidate, n_epochs=20)\n",
    "\n",
    "            # Rec #13: Invariant checks after adjustment\n",
    "            self._check_invariants(candidate)\n",
    "\n",
    "            # Evaluate using composite objective\n",
    "            metrics = self.evaluate(candidate)\n",
    "            score = objective.compute(metrics)\n",
    "\n",
    "            # Track history\n",
    "            self.history[\"mse\"].append(metrics.mse)\n",
    "            self.history[\"kl\"].append(metrics.kl_divergence)\n",
    "            self.history[\"tv\"].append(metrics.total_variation)\n",
    "            self.history[\"composite\"].append(score)\n",
    "\n",
    "            self.log(f\"\\n--- ITERATION {it} ---\")\n",
    "            self.log(\n",
    "                f\"  KL={metrics.kl_divergence:.6f}, TV={metrics.total_variation:.6f}, \"\n",
    "                f\"Score={score:.6f}\"\n",
    "            )\n",
    "\n",
    "            # Log conditional metrics if any violations\n",
    "            if metrics.conditional_tv:\n",
    "                violations = {\n",
    "                    k: v\n",
    "                    for k, v in metrics.conditional_tv.items()\n",
    "                    if v > objective.conditional_tolerance\n",
    "                }\n",
    "                if violations:\n",
    "                    self.log(f\"  Conditional violations: {violations}\")\n",
    "\n",
    "            if score < best_score:\n",
    "                self.log(\"  [ACCEPTED]\")\n",
    "                best_df = candidate\n",
    "                best_metrics = metrics\n",
    "                best_score = score\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                self.log(\"  [REJECTED]\")\n",
    "                no_improvement_count += 0\n",
    "\n",
    "            # Rec #10: Early exit on no-op (no improvement for several iterations)\n",
    "            if no_improvement_count >= 5:\n",
    "                self.log(\"  [EARLY EXIT] No improvement for 5 iterations\")\n",
    "                break\n",
    "\n",
    "            if best_score <= tolerance:\n",
    "                self.log(\"  [CONVERGED]\")\n",
    "                break\n",
    "\n",
    "        self.log(\n",
    "            f\"\\n[FINAL] KL={best_metrics.kl_divergence:.6f}, \"\n",
    "            f\"TV={best_metrics.total_variation:.6f}, Score={best_score:.6f}\"\n",
    "        )\n",
    "        self._print_final_summary(best_df, best_metrics)\n",
    "\n",
    "        return best_df\n",
    "\n",
    "    def _check_invariants(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Assert invariants after each iteration (Rec #13).\n",
    "\n",
    "        Fail fast on silent corruption rather than propagating errors.\n",
    "        \"\"\"\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "            false_val = col_cfg[\"values\"][\"false_value\"]\n",
    "\n",
    "            # Check all values are valid binary\n",
    "            valid_values = df[col_id].isin([true_val, false_val])\n",
    "            if not bool(valid_values.all()):\n",
    "                invalid_vals = df.loc[~valid_values, col_id].unique().tolist()\n",
    "                raise ValueError(\n",
    "                    f\"Invariant violation: {col_id} contains invalid values. \"\n",
    "                    f\"Expected {true_val} or {false_val}, got: {invalid_vals}\"\n",
    "                )\n",
    "\n",
    "            # Check probability is in valid range (implicit, but verify no NaN)\n",
    "            if bool(df[col_id].isna().any()):\n",
    "                raise ValueError(f\"Invariant violation: {col_id} contains NaN values\")\n",
    "\n",
    "    def _print_final_summary(self, df: pd.DataFrame, metrics: GeneratorMetrics):\n",
    "        \"\"\"Print final summary of generation results.\"\"\"\n",
    "        self.log(\"\\n[FINAL DISTRIBUTION SUMMARY]\")\n",
    "        for col_cfg in self.config[\"columns\"]:\n",
    "            col_id = col_cfg[\"column_id\"]\n",
    "            true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "            target = col_cfg[\"distribution\"][\"probabilities\"][\"true_prob\"]\n",
    "            observed = (df[col_id] == true_val).mean()\n",
    "            error = metrics.per_column_errors[col_id]\n",
    "            self.log(\n",
    "                f\"  {col_id}: target={target:.3f}, observed={observed:.3f}, error={error:.4f}\"\n",
    "            )\n",
    "\n",
    "    def generate(self, n_rows: int) -> pd.DataFrame:\n",
    "        \"\"\"Full generation pipeline.\"\"\"\n",
    "        self.log(f\"\\n[GENERATING {n_rows} ROWS]\")\n",
    "\n",
    "        # Step 1: Initial generation\n",
    "        df = self.generate_initial_dataset(n_rows)\n",
    "\n",
    "        # Step 2: Apply conditional dependencies\n",
    "        df = self.adjuster._enforce_conditional_dependencies(df)\n",
    "\n",
    "        # Step 3: Optimize\n",
    "        df = self.optimize(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Top-Level API Functions (Rec #15)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def optimize_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    config: dict,\n",
    "    objective: Optional[OptimizationObjective] = None,\n",
    "    max_iters: int = 50,\n",
    "    tolerance: float = 1e-6,\n",
    "    use_attention: bool = True,\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[pd.DataFrame, GeneratorMetrics]:\n",
    "    \"\"\"\n",
    "    Optimize an existing dataset to match target distributions (Rec #15).\n",
    "\n",
    "    This is the recommended entry point for distribution repair/optimization.\n",
    "    The system excels at repairing distributions, not just initial sampling.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame to optimize\n",
    "        config: YAML configuration dictionary with target distributions\n",
    "        objective: Custom optimization objective (uses default if None)\n",
    "        max_iters: Maximum optimization iterations\n",
    "        tolerance: Convergence tolerance for objective score\n",
    "        use_attention: Whether to use attention-based proposal model\n",
    "        seed: Random seed for reproducibility\n",
    "        verbose: Whether to print progress messages\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (optimized_df, final_metrics)\n",
    "\n",
    "    Example:\n",
    "        >>> config = load_config(CONFIG_YAML)\n",
    "        >>> df_optimized, metrics = optimize_dataset(\n",
    "        ...     df=my_dataset,\n",
    "        ...     config=config,\n",
    "        ...     objective=OptimizationObjective(weight_kl=0.5, weight_tv=0.5),\n",
    "        ... )\n",
    "        >>> print(f\"Final KL: {metrics.kl_divergence:.6f}\")\n",
    "    \"\"\"\n",
    "    generator = ImprovedBinaryGenerator(\n",
    "        config=config,\n",
    "        seed=seed,\n",
    "        use_attention=use_attention,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    optimized_df = generator.optimize(\n",
    "        df=df,\n",
    "        max_iters=max_iters,\n",
    "        tolerance=tolerance,\n",
    "        objective=objective,\n",
    "    )\n",
    "\n",
    "    final_metrics = generator.evaluate(optimized_df)\n",
    "\n",
    "    return optimized_df, final_metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Utility Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_config(yaml_str: str) -> dict:\n",
    "    \"\"\"Load YAML config and print basic metadata.\"\"\"\n",
    "    config = yaml.safe_load(yaml_str)\n",
    "\n",
    "    print(\"\\n[CONFIG LOADED]\")\n",
    "    print(f\"  Name: {config['metadata']['name']}\")\n",
    "    print(f\"  Version: {config['metadata']['version']}\")\n",
    "    print(f\"  Columns: {len(config['columns'])}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def validate_distribution(df: pd.DataFrame, config: dict) -> Dict:\n",
    "    \"\"\"Validate that generated data matches config probabilities.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for col_cfg in config[\"columns\"]:\n",
    "        col_id = col_cfg[\"column_id\"]\n",
    "        true_val = col_cfg[\"values\"][\"true_value\"]\n",
    "        target = col_cfg[\"distribution\"][\"probabilities\"][\"true_prob\"]\n",
    "        observed = (df[col_id] == true_val).mean()\n",
    "\n",
    "        results[col_id] = {\n",
    "            \"target\": target,\n",
    "            \"observed\": observed,\n",
    "            \"error\": abs(target - observed),\n",
    "            \"passed\": abs(target - observed) < 0.05,  # 5% tolerance\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Demo Configuration (same as notebook)\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG_YAML = \"\"\"\n",
    "      # Binary Configuration for Sector Retail Shop\n",
    "      # Retail shop operational binary variables\n",
    "\n",
    "      metadata:\n",
    "        name: \"sector_retail_shop_binary\"\n",
    "        version: \"1.0.0\"\n",
    "        description: \"Binary variables for retail shop operations\"\n",
    "        author: \"Retail Analytics Team\"\n",
    "        created_date: \"2025-01-19\"\n",
    "        modified_date: \"2025-01-19\"\n",
    "        dataset_id: \"sector_retail_shop\"\n",
    "        config_type: \"binary\"\n",
    "        schema_version: \"2.0\"\n",
    "\n",
    "      columns:\n",
    "        # Loyalty membership flag\n",
    "        - column_id: \"has_loyalty_card\"\n",
    "          column_name: \"HasLoyaltyCard\"\n",
    "          column_description: \"Whether customer has a loyalty card membership\"\n",
    "          \n",
    "          data_type:\n",
    "            type: \"binary\"\n",
    "            representation: \"integer\"\n",
    "            \n",
    "          values:\n",
    "            true_value: 1\n",
    "            false_value: 0\n",
    "            labels:\n",
    "              true_label: \"Member\"\n",
    "              false_label: \"Non-Member\"\n",
    "              \n",
    "          distribution:\n",
    "            type: \"bernoulli\"\n",
    "            probabilities:\n",
    "              true_prob: 0.35\n",
    "              false_prob: 0.65\n",
    "              \n",
    "            mutation:\n",
    "              enabled: false\n",
    "              rate: 0.0\n",
    "              \n",
    "          generation:\n",
    "            global: true\n",
    "            missing_value_rate: 0.0\n",
    "            missing_value: null\n",
    "            \n",
    "          validation:\n",
    "            required: true\n",
    "            unique: false\n",
    "            balance_threshold: null\n",
    "            \n",
    "        # Discount applied flag\n",
    "        - column_id: \"discount_applied\"\n",
    "          column_name: \"DiscountApplied\"\n",
    "          column_description: \"Whether a discount was applied to the transaction\"\n",
    "          \n",
    "          data_type:\n",
    "            type: \"binary\"\n",
    "            representation: \"integer\"\n",
    "            \n",
    "          values:\n",
    "            true_value: 1\n",
    "            false_value: 0\n",
    "            labels:\n",
    "              true_label: \"Discounted\"\n",
    "              false_label: \"Full Price\"\n",
    "              \n",
    "          distribution:\n",
    "            type: \"conditional\"\n",
    "            \n",
    "            probabilities:\n",
    "              true_prob: 0.20\n",
    "              false_prob: 0.80\n",
    "              \n",
    "            dependencies:\n",
    "              depend_on: [\"has_loyalty_card\"]\n",
    "              conditional_probs:\n",
    "                \"has_loyalty_card=1\": { true_prob: 0.65, false_prob: 0.35 }\n",
    "                \"has_loyalty_card=0\": { true_prob: 0.15, false_prob: 0.85 }\n",
    "                \n",
    "            mutation:\n",
    "              enabled: false\n",
    "              rate: 0.0\n",
    "              \n",
    "          generation:\n",
    "            global: true\n",
    "            missing_value_rate: 0.0\n",
    "            missing_value: null\n",
    "            \n",
    "          validation:\n",
    "            required: true\n",
    "            unique: false\n",
    "            balance_threshold: null\n",
    "            \n",
    "        # Online order flag\n",
    "        - column_id: \"is_online_order\"\n",
    "          column_name: \"IsOnlineOrder\"\n",
    "          column_description: \"Whether the order was placed online\"\n",
    "          \n",
    "          data_type:\n",
    "            type: \"binary\"\n",
    "            representation: \"integer\"\n",
    "            \n",
    "          values:\n",
    "            true_value: 1\n",
    "            false_value: 0\n",
    "            labels:\n",
    "              true_label: \"Online\"\n",
    "              false_label: \"In-Store\"\n",
    "              \n",
    "          distribution:\n",
    "            type: \"bernoulli\"\n",
    "            probabilities:\n",
    "              true_prob: 0.42\n",
    "              false_prob: 0.58\n",
    "              \n",
    "            mutation:\n",
    "              enabled: false\n",
    "              rate: 0.0\n",
    "              \n",
    "          generation:\n",
    "            global: true\n",
    "            missing_value_rate: 0.0\n",
    "            missing_value: null\n",
    "            \n",
    "          validation:\n",
    "            required: true\n",
    "            unique: false\n",
    "            balance_threshold: null\n",
    "            \n",
    "        # Return flag\n",
    "        - column_id: \"is_return\"\n",
    "          column_name: \"IsReturn\"\n",
    "          column_description: \"Whether the transaction is a return\"\n",
    "          \n",
    "          data_type:\n",
    "            type: \"binary\"\n",
    "            representation: \"integer\"\n",
    "            \n",
    "          values:\n",
    "            true_value: 1\n",
    "            false_value: 0\n",
    "            labels:\n",
    "              true_label: \"Return\"\n",
    "              false_label: \"Purchase\"\n",
    "              \n",
    "          distribution:\n",
    "            type: \"conditional\"\n",
    "            \n",
    "            probabilities:\n",
    "              true_prob: 0.08\n",
    "              false_prob: 0.92\n",
    "              \n",
    "            dependencies:\n",
    "              depend_on: [\"is_online_order\", \"has_loyalty_card\"]\n",
    "              conditional_probs:\n",
    "                \"is_online_order=1, has_loyalty_card=1\": { true_prob: 0.12, false_prob: 0.88 }\n",
    "                \"is_online_order=1, has_loyalty_card=0\": { true_prob: 0.05, false_prob: 0.95 }\n",
    "                \"is_online_order=0, has_loyalty_card=1\": { true_prob: 0.07, false_prob: 0.93 }\n",
    "                \"is_online_order=0, has_loyalty_card=0\": { true_prob: 0.04, false_prob: 0.96 }\n",
    "                \n",
    "            mutation:\n",
    "              enabled: false\n",
    "              rate: 0.0\n",
    "              \n",
    "          generation:\n",
    "            global: false\n",
    "            missing_value_rate: 0.0\n",
    "            missing_value: null\n",
    "            \n",
    "          validation:\n",
    "            required: true\n",
    "            unique: false\n",
    "            balance_threshold: null\n",
    "\n",
    "      template_config:\n",
    "        validate_probabilities: true\n",
    "        validate_logic: true\n",
    "        validate_dependencies: true\n",
    "        \n",
    "        defaults:\n",
    "          representation: \"integer\"\n",
    "          true_value: 1\n",
    "          false_value: 0\n",
    "          missing_value: null\n",
    "          mutation_rate: 0.01\n",
    "          \n",
    "        processing_hints:\n",
    "          optimize_memory: true\n",
    "          parallelize: true\n",
    "          cache_logical: true\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Entry Point\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load configuration\n",
    "    config = load_config(CONFIG_YAML)\n",
    "\n",
    "    # Create generator with all improvements\n",
    "    generator = ImprovedBinaryGenerator(\n",
    "        config=config, seed=42, use_attention=True, learning_rate=0.01, verbose=True\n",
    "    )\n",
    "\n",
    "    # Generate dataset\n",
    "    ROWS = 1000\n",
    "    final_df = generator.generate(ROWS)\n",
    "\n",
    "    print(\"\\n[FINAL DATASET SAMPLE]\")\n",
    "    print(final_df.head(10))\n",
    "\n",
    "    print(\"\\n[VALUE COUNTS PER COLUMN]\")\n",
    "    for col in final_df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(final_df[col].value_counts())\n",
    "\n",
    "    # Validate results\n",
    "    print(\"\\n[VALIDATION RESULTS]\")\n",
    "    validation = validate_distribution(final_df, config)\n",
    "    for col_id, result in validation.items():\n",
    "        status = \"PASS\" if result[\"passed\"] else \"FAIL\"\n",
    "        print(\n",
    "            f\"  {col_id}: target={result['target']:.3f}, observed={result['observed']:.3f}, [{status}]\"\n",
    "        )\n",
    "\n",
    "    # Show metrics comparison\n",
    "    print(\"\\n[METRICS COMPARISON]\")\n",
    "    metrics = generator.evaluate(final_df)\n",
    "    print(f\"  MSE: {metrics.mse:.6f}\")\n",
    "    print(f\"  KL Divergence: {metrics.kl_divergence:.6f}\")\n",
    "    print(f\"  Total Variation: {metrics.total_variation:.6f}\")\n",
    "    print(f\"  Max Deviation: {metrics.max_deviation:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba20f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "except:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e919d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd26e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'dict'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'type'>\n",
      "Hello World\n",
      "<class 'NoneType'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'bool'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "class absClass:\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "def hello_world():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "\n",
    "x = 19\n",
    "dict = {1: 1, 2: 2}\n",
    "string = \"Hello_World\"\n",
    "flo = 1.23456\n",
    "arr = [1,2,3,4,5]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vec1 = np.array([1, 2, 3, 4])\n",
    "vec2 = np.array([[1, 2, 3, 4],\n",
    "                [1, 1, 1, 1]])\n",
    "\n",
    "log = True\n",
    "\n",
    "chto = (1,2,3,4,5)\n",
    "\n",
    "print(type(x))\n",
    "print(type(dict))\n",
    "print(type(string))\n",
    "print(type(flo))\n",
    "print(type(absClass))\n",
    "print(type(hello_world()))\n",
    "print(type(df))\n",
    "print(type(arr))\n",
    "print(type(vec1))\n",
    "print(type(vec2))\n",
    "print(type(log))\n",
    "print(type(chto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PDF_Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915f9af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs 3\n",
      "cdbdsa 6\n",
      "qqqqqqq 7\n",
      "123123123 9\n"
     ]
    }
   ],
   "source": [
    "words = ['abs', 'cdbdsa', 'qqqqqqq', '123123123']\n",
    "\n",
    "for word in words:\n",
    "    print(word, len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80dd07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****_****_1234_4321 Your card is active\n",
      "****_****_6712_0920 Your card is inactive please top-up your balance\n",
      "****_****_1221_0099 Your card is active\n"
     ]
    }
   ],
   "source": [
    "card_data = {'****_****_1234_4321': \"active\", '****_****_6712_0920': \"inactive\", '****_****_1221_0099': \"active\"}\n",
    "\n",
    "for card, status in card_data.copy().items():\n",
    "    if status == \"inactive\":\n",
    "        print(card + \" Your card is inactive please top-up your balance\")\n",
    "    else:\n",
    "        print(card + \" Your card is active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173fb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
